{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model subclassing and custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Coding tutorials\n",
    " #### [1. Model subclassing](#coding_tutorial_1)\n",
    " #### [2. Custom layers](#coding_tutorial_2)\n",
    " #### [3. Automatic differentiation](#coding_tutorial_3)\n",
    " #### [4. Custom training loops](#coding_tutorial_4)\n",
    " #### [5. tf.function decorator](#coding_tutorial_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple model using the model subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "class MyModel(Model):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dense2 = Dense(10, activation='linear')\n",
    "        self.dropout = Dropout(0.4)\n",
    "        self.dense3 = Dense(5)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        \n",
    "        x = self.dense1(inputs)\n",
    "        y1 = self.dense2(inputs)\n",
    "        y2 = self.dense3(y1)\n",
    "        concat = concatenate([x, y2])\n",
    "                \n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        return self.softmax(concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [1,64], In[1]: [10,10] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-42d781b1fa52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-628b72d3ef7e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6124\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6126\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6127\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [1,64], In[1]: [10,10] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model = MyModel()\n",
    "model(tf.random.uniform((1,10)))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.1714879  -0.0565123   0.09953833]], shape=(1, 3), dtype=float32)\n",
      "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-0.01070861, -0.07876535, -0.03072574],\n",
      "       [-0.03150852, -0.02584002,  0.02394998],\n",
      "       [-0.10052462, -0.03423536,  0.05023601],\n",
      "       [-0.03350559,  0.06483435,  0.01213694],\n",
      "       [ 0.00475945,  0.01749409,  0.04394113]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n",
      "trainable weights: 2\n"
     ]
    }
   ],
   "source": [
    "# Create a custom layer\n",
    "class MyLayer(Layer):\n",
    "    \n",
    "    def __init__(self, units, input_dim):\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units), initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(units,), initializer='zeros')\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "dense_layer = MyLayer(3, 5)\n",
    "x = tf.ones((1,5))\n",
    "print(dense_layer(x))\n",
    "print(dense_layer.weights)\n",
    "print(f\"trainable weights: {len(dense_layer.trainable_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.3070693  -0.02422041 -0.09420305]], shape=(1, 3), dtype=float32)\n",
      "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-0.01701525,  0.0189072 ,  0.03508155],\n",
      "       [ 0.00080478, -0.01853837, -0.02392031],\n",
      "       [-0.10192958, -0.0215773 , -0.03922046],\n",
      "       [-0.06650129, -0.02162644,  0.01490488],\n",
      "       [-0.12242796,  0.01861451, -0.0810487 ]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n",
      "trainable weights: 0\n"
     ]
    }
   ],
   "source": [
    "# Specify trainable weights\n",
    "class MyLayer(Layer):\n",
    "    \n",
    "    def __init__(self, units, input_dim):\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units), initializer='random_normal', trainable=False)\n",
    "        self.b = self.add_weight(shape=(units,), initializer='zeros', trainable=False)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "dense_layer = MyLayer(3, 5)\n",
    "x = tf.ones((1,5))\n",
    "print(dense_layer(x))\n",
    "print(dense_layer.weights)\n",
    "print(f\"trainable weights: {len(dense_layer.trainable_weights)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 0\n",
      "non-trainable weights: 2\n"
     ]
    }
   ],
   "source": [
    "print('trainable weights:', len(dense_layer.trainable_weights))\n",
    "print('non-trainable weights:', len(dense_layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=193, shape=(1, 3), dtype=float32, numpy=array([[-0.02220715, -0.24516505,  0.10130763]], dtype=float32)>, <tf.Tensor: id=200, shape=(3,), dtype=float32, numpy=array([-0.02220715, -0.24516505,  0.10130763], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Create a custom layer to accumulate means of output values\n",
    "\n",
    "class MyLayerMean(Layer):\n",
    "    \n",
    "    def __init__(self, units, input_dim):\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units), initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(units,), initializer='zeros')\n",
    "        \n",
    "        self.sum_activation  =tf.Variable(initial_value=tf.zeros((units,)))\n",
    "        self.number_call = tf.Variable(initial_value=0, trainable=False)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        activations = tf.matmul(inputs, self.w) + self.b\n",
    "        self.sum_activation.assign_add(tf.reduce_sum(activations, axis=0))\n",
    "        self.number_call.assign_add(inputs.shape[0])\n",
    "        return activations, self.sum_activation / tf.cast(self.number_call, tf.float32)\n",
    "\n",
    "dense_layer = MyLayerMean(3, 5)\n",
    "x = tf.ones((1,5))\n",
    "print(dense_layer(x))\n",
    "# print(dense_layer.weights)\n",
    "# print(f\"trainable weights: {len(dense_layer.trainable_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02220715 -0.24516505  0.10130763]\n"
     ]
    }
   ],
   "source": [
    "# Test the layer\n",
    "\n",
    "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
    "print(activation_means.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dropout layer as a custom layer\n",
    "\n",
    "class MyDropout(Layer):\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Define forward pass for dropout layer\n",
    "        return tf.nn.dropout(inputs, rate=self.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the custom layers into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using custom layers with the model subclassing API\n",
    "\n",
    "class MyModel(Model):\n",
    "\n",
    "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.layer_1 = MyLayer(units_1, input_dim_1)\n",
    "        self.dropout_1 = MyDropout(0.5)\n",
    "        self.layer_2 = MyLayer(units_2, units_1)\n",
    "        self.dropout_2 = MyDropout(0.5)\n",
    "        self.layer_3 = MyLayer(units_3, units_2)\n",
    "        self.softmax = Softmax()\n",
    "           \n",
    "    def call(self, inputs):\n",
    "        # Define forward pass\n",
    "        x = self.layer_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.00367461 0.00533872 0.11202089 0.00469375 0.04500173 0.02536586\n",
      "  0.05257694 0.00776247 0.00579904 0.0276773  0.0012261  0.02075638\n",
      "  0.03132445 0.01770858 0.00948754 0.00845671 0.0587641  0.00584114\n",
      "  0.00550522 0.0118585  0.0068082  0.01396714 0.01038253 0.05928208\n",
      "  0.00550036 0.02832863 0.0402202  0.01273454 0.01192612 0.01347064\n",
      "  0.01958467 0.00821592 0.00840725 0.01686658 0.00957038 0.06128845\n",
      "  0.04093413 0.07753836 0.01807537 0.00333505 0.00918688 0.0078111\n",
      "  0.01867516 0.00728352 0.0057281  0.02403857]], shape=(1, 46), dtype=float32)\n",
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_2 (MyLayer)         multiple                  640064    \n",
      "_________________________________________________________________\n",
      "my_dropout (MyDropout)       multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_3 (MyLayer)         multiple                  4160      \n",
      "_________________________________________________________________\n",
      "my_dropout_1 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_4 (MyLayer)         multiple                  2990      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 0\n",
      "Non-trainable params: 647,214\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a model object\n",
    "\n",
    "model = MyModel(64,10000,64,46)\n",
    "print(model(tf.ones((1, 10000))))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2e301ec320>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data from a noise contaminated linear model\n",
    "\n",
    "def MakeNoisyData(m, b, n=20):\n",
    "    x = tf.random.uniform(shape=(n,))\n",
    "    noise = tf.random.normal(shape=(len(x),), stddev=0.1)\n",
    "    y = m * x + b + noise\n",
    "    return x, y\n",
    "\n",
    "m=1\n",
    "b=2\n",
    "x_train, y_train = MakeNoisyData(m,b)\n",
    "plt.plot(x_train, y_train, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.00512527 -0.00626083 -0.01536766 -0.01423403 -0.0006964  -0.00586463\n",
      " -0.01439946 -0.01430376 -0.00754058 -0.00470976 -0.01548238 -0.01356593\n",
      " -0.01194949 -0.01438915 -0.01151854 -0.00547433 -0.0088794  -0.00062335\n",
      " -0.00563915 -0.00182114], shape=(20,), dtype=float32)\n",
      "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-0.01613555], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# Build a custom layer for the linear regression model\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = self.add_weight(shape=(1,), initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(1,), initializer='zeros')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.m * inputs + self.b\n",
    "    \n",
    "linear_regression = LinearLayer()\n",
    "\n",
    "print(linear_regression(x_train))\n",
    "print(linear_regression.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss 6.697608\n"
     ]
    }
   ],
   "source": [
    "# Define the mean squared error loss function\n",
    "\n",
    "def SquaredError(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
    "\n",
    "starting_loss = SquaredError(linear_regression(x_train), y_train)\n",
    "print(\"Starting loss\", starting_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 25, loss: 6.69760799407959\n",
      "steps: 25, loss: 5.040121078491211\n",
      "steps: 25, loss: 3.793442487716675\n",
      "steps: 25, loss: 2.8557534217834473\n",
      "steps: 25, loss: 2.1504693031311035\n",
      "steps: 25, loss: 1.6199886798858643\n",
      "steps: 25, loss: 1.2209861278533936\n",
      "steps: 25, loss: 0.9208744764328003\n",
      "steps: 25, loss: 0.6951436400413513\n",
      "steps: 25, loss: 0.5253580808639526\n",
      "steps: 25, loss: 0.3976518511772156\n",
      "steps: 25, loss: 0.3015953600406647\n",
      "steps: 25, loss: 0.2293442189693451\n",
      "steps: 25, loss: 0.17499831318855286\n",
      "steps: 25, loss: 0.13411985337734222\n",
      "steps: 25, loss: 0.10337083041667938\n",
      "steps: 25, loss: 0.08024080097675323\n",
      "steps: 25, loss: 0.06284128129482269\n",
      "steps: 25, loss: 0.04975210875272751\n",
      "steps: 25, loss: 0.039904940873384476\n",
      "steps: 25, loss: 0.032496266067028046\n",
      "steps: 25, loss: 0.026921767741441727\n",
      "steps: 25, loss: 0.022726813331246376\n",
      "steps: 25, loss: 0.019569534808397293\n",
      "steps: 25, loss: 0.017192788422107697\n"
     ]
    }
   ],
   "source": [
    "# Implement a gradient descent training loop for the linear regression model\n",
    "learning_rate = 0.05\n",
    "steps = 25\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = linear_regression(x_train)\n",
    "        loss = SquaredError(predictions, y_train)\n",
    "    \n",
    "    gradients = tape.gradient(loss, linear_regression.trainable_variables)\n",
    "    \n",
    "    linear_regression.m.assign_sub(learning_rate * gradients[0])\n",
    "    linear_regression.b.assign_sub(learning_rate * gradients[1])\n",
    "    print(f\"steps: {steps}, loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:1,  trained m:[1.0823308]\n",
      "b:2,  trained b:[1.8839844]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2e3011f2b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learned regression model\n",
    "\n",
    "print(\"m:{},  trained m:{}\".format(m,linear_regression.m.numpy()))\n",
    "print(\"b:{},  trained b:{}\".format(b,linear_regression.b.numpy()))\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.')\n",
    "\n",
    "x_linear_regression=np.linspace(min(x_train), max(x_train),50)\n",
    "plt.plot(x_linear_regression, linear_regression.m*x_linear_regression+linear_regression.b, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## Custom training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom layers and model\n",
    "class MyLayer(Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=False, name='kernel')\n",
    "        self.b = self.add_weight(shape=(self.units, ), initializer='zeros', trainable=False, name='bias')\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "class MyDropout(Layer):\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Define forward pass for dropout layer\n",
    "        return tf.nn.dropout(inputs, rate=self.rate)\n",
    "\n",
    "class MyModel(Model):\n",
    "\n",
    "    def __init__(self, units_1, units_2, units_3):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.layer_1 = MyLayer(units_1)\n",
    "        self.dropout_1 = MyDropout(0.5)\n",
    "        self.layer_2 = MyLayer(units_2)\n",
    "        self.dropout_2 = MyDropout(0.5)\n",
    "        self.layer_3 = MyLayer(units_3)\n",
    "        self.softmax = Softmax()\n",
    "           \n",
    "    def call(self, inputs):\n",
    "        # Define forward pass\n",
    "        x = self.layer_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the reuters dataset and define the class_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.02391133 0.02271424 0.01798472 0.03862394 0.02650401 0.03192272\n",
      "  0.01774638 0.03088432 0.01946568 0.02021451 0.03536893 0.03097508\n",
      "  0.00958112 0.01744058 0.02220442 0.02217767 0.01118959 0.02393745\n",
      "  0.013365   0.02398203 0.02213743 0.01679549 0.01650853 0.00856026\n",
      "  0.03008369 0.02517031 0.01103467 0.01806214 0.01406382 0.00785418\n",
      "  0.01659579 0.01399389 0.01128633 0.02701059 0.0289167  0.04036261\n",
      "  0.02361255 0.04264146 0.02207304 0.01318961 0.01455322 0.02019591\n",
      "  0.02649036 0.02033632 0.03531584 0.01296159]], shape=(1, 46), dtype=float32)\n",
      "Model: \"my_model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_15 (MyLayer)        multiple                  64064     \n",
      "_________________________________________________________________\n",
      "my_dropout_8 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_16 (MyLayer)        multiple                  4160      \n",
      "_________________________________________________________________\n",
      "my_dropout_9 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_17 (MyLayer)        multiple                  2990      \n",
      "_________________________________________________________________\n",
      "softmax_5 (Softmax)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 71,214\n",
      "Trainable params: 0\n",
      "Non-trainable params: 71,214\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instaniate the model object\n",
    "model = MyModel(64, 64, 46)\n",
    "print(model(tf.ones((1,1000))))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: earn\n"
     ]
    }
   ],
   "source": [
    "# Print the class of the first sample\n",
    "\n",
    "print(\"Label: {}\".format(class_names[train_labels[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Reuters word index\n",
    "\n",
    "word_to_index = reuters.get_word_index()\n",
    "\n",
    "invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])\n",
    "text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "# Print the first data example sentence\n",
    "print(text_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (8982, 10000)\n",
      "Shape of x_test: (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Define a function that encodes the data into a 'bag of words' representation\n",
    "\n",
    "def bag_of_words(text_samples, elements=10000):\n",
    "    output = np.zeros((len(text_samples), elements))\n",
    "    for i, word in enumerate(text_samples):\n",
    "        output[i, word] = 1.\n",
    "    return output\n",
    "\n",
    "x_train = bag_of_words(train_data)\n",
    "x_test = bag_of_words(test_data)\n",
    "\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical cross entropy loss and Adam optimizer\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss(model, x, y, wd):\n",
    "    kernel_variables = []\n",
    "    for l in model.layers:\n",
    "        for w in l.weights:\n",
    "            if 'kernel' in w.name:\n",
    "                kernel_variables.append(w)\n",
    "    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_) + wd_penalty\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the forward and backward pass\n",
    "\n",
    "def grad(model, inputs, targets, wd):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, wd)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [32,10000], In[1]: [1000,64] [Op:MatMul] name: my_model_6/my_layer_15/MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-4fd8265ac3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-2b7db94d689b>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(model, inputs, targets, wd)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8627af2b2268>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(model, x, y, wd)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mkernel_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mwd_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwd_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-79444a950c4e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Define forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-79444a950c4e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2763\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2765\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6124\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6126\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6127\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [32,10000], In[1]: [1000,64] [Op:MatMul] name: my_model_6/my_layer_15/MatMul/"
     ]
    }
   ],
   "source": [
    "# Implement the training loop\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "start_time = time.time()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 10\n",
    "weight_decay = 0.005\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    #training loop\n",
    "    for x, y in train_dataset:\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y, weight_decay)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        #compute current loss\n",
    "        epoch_loss_avg(loss_value)\n",
    "        #compare predicted label to actual label\n",
    "        epoch_accuracy(to_categorical(y), model(x))\n",
    "    \n",
    "    #End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    print(\"epoch: {:03d} loss: {:.3f} Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "    \n",
    "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset object for the test set\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect average loss and accuracy\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [32,10000], In[1]: [1000,64] [Op:MatMul] name: my_model_6/my_layer_15/MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-52baa918b9ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Compute current loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_loss_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8627af2b2268>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(model, x, y, wd)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mkernel_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mwd_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwd_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-79444a950c4e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Define forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-79444a950c4e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2763\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2765\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6124\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6126\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6127\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [32,10000], In[1]: [1000,64] [Op:MatMul] name: my_model_6/my_layer_15/MatMul/"
     ]
    }
   ],
   "source": [
    "# Loop over the test set and print scores\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "for x, y in test_dataset:\n",
    "    # Optimize the model\n",
    "    loss_value = loss(model, x, y, weight_decay)    \n",
    "    # Compute current loss\n",
    "    epoch_loss_avg(loss_value)  \n",
    "    # Compare predicted label to actual label\n",
    "    epoch_accuracy(to_categorical(y), model(x))\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
    "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c95e54902442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss_results' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss and accuracy\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model prediction for an example input\n",
    "\n",
    "predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]\n",
    "print(\"Prediction: {}\".format(class_names[predicted_label]))\n",
    "print(\"     Label: {}\".format(class_names[train_labels[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## tf.function decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model\n",
    "model = MyModel(64, 64, 46 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine the grad function using the @tf.function decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the @tf.function decorator\n",
    "@tf.function\n",
    "def grad(model, inputs, targets, wd):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, wd)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch: 000 loss: 11.924 Accuracy: 1.681%\n",
      "epoch: 001 loss: 11.924 Accuracy: 1.904%\n",
      "epoch: 002 loss: 11.925 Accuracy: 1.926%\n",
      "epoch: 003 loss: 11.924 Accuracy: 1.726%\n",
      "epoch: 004 loss: 11.925 Accuracy: 1.915%\n",
      "epoch: 005 loss: 11.926 Accuracy: 1.804%\n",
      "epoch: 006 loss: 11.925 Accuracy: 1.603%\n",
      "epoch: 007 loss: 11.924 Accuracy: 1.848%\n",
      "epoch: 008 loss: 11.926 Accuracy: 1.670%\n",
      "epoch: 009 loss: 11.924 Accuracy: 2.071%\n",
      "Duration :99.680\n"
     ]
    }
   ],
   "source": [
    "# Re-run the training loop\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "start_time = time.time()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, train_labels))\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 10\n",
    "weight_decay = 0.005\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    #training loop\n",
    "    for x, y in train_dataset:\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y, weight_decay)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        #compute current loss\n",
    "        epoch_loss_avg(loss_value)\n",
    "        #compare predicted label to actual label\n",
    "        epoch_accuracy(to_categorical(y), model(x))\n",
    "    \n",
    "    #End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    print(\"epoch: {:03d} loss: {:.3f} Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
    "    \n",
    "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the autograph code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__grad(model, inputs, targets, wd):\n",
      "  do_return = False\n",
      "  retval_ = ag__.UndefinedReturnValue()\n",
      "  with ag__.FunctionScope('grad', 'grad_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as grad_scope:\n",
      "    with tf.GradientTape() as tape:\n",
      "      loss_value = ag__.converted_call(loss, grad_scope.callopts, (model, inputs, targets, wd), None, grad_scope)\n",
      "    do_return = True\n",
      "    retval_ = grad_scope.mark_return_value((loss_value, ag__.converted_call(tape.gradient, grad_scope.callopts, (loss_value, model.trainable_variables), None, grad_scope)))\n",
      "  do_return,\n",
      "  return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf.autograph.to_code to see the generated code\n",
    "print(tf.autograph.to_code(grad.python_function))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
